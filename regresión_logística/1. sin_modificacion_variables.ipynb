{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Logística - Pair programming ###\n",
    "\n",
    "## 1. EDA ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "# -----------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Gráficos\n",
    "# ------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Estandarización variables numéricas y Codificación variables categóricas\n",
    "# -----------------------------------------------------------------------------\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Gestión datos desbalanceados\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score , cohen_kappa_score, roc_curve,roc_auc_score\n",
    "\n",
    "# Para separar los datos en train y test\n",
    "# ------------------------------------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#  Gestión de warnings\n",
    "# ------------------------------------------------------------------------------\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los ejercicios de pair programming de Regresión logística tendremos que buscar un dataset (al igual que hicismos en regresión lineal) que usaremos a lo largo de los siguientes ejercicios.\n",
    "\n",
    "Se ruega a la hora de realizar la entrega que incluyais el conjunto de datos que hayais decidido emplear para estos ejercicios.\n",
    "\n",
    "Objetivos\n",
    "\n",
    "Buscar un conjunto de datos a analizar\n",
    "\n",
    "Se recomienda que el conjunto de datos a analizar tenga variables numéricas y categóricas, primando que haya más de una variable de tipo numérico.\n",
    "\n",
    "Explicar los datos y las variables disponibles en el conjunto de datos seleccionado\n",
    "\n",
    "Realizar un EDA sencillo poniendo en práctica los conocimientos adquiridos hasta el momento.\n",
    "\n",
    "Interpretación de los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57.877857</td>\n",
       "      <td>0.311140</td>\n",
       "      <td>1.945940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.829943</td>\n",
       "      <td>0.175592</td>\n",
       "      <td>1.294219</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.091079</td>\n",
       "      <td>0.805153</td>\n",
       "      <td>0.427715</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.247564</td>\n",
       "      <td>5.600044</td>\n",
       "      <td>0.362663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.190936</td>\n",
       "      <td>0.566486</td>\n",
       "      <td>2.222767</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   distance_from_home  distance_from_last_transaction  \\\n",
       "0           57.877857                        0.311140   \n",
       "1           10.829943                        0.175592   \n",
       "2            5.091079                        0.805153   \n",
       "3            2.247564                        5.600044   \n",
       "4           44.190936                        0.566486   \n",
       "\n",
       "   ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
       "0                        1.945940              1.0        1.0   \n",
       "1                        1.294219              1.0        0.0   \n",
       "2                        0.427715              1.0        0.0   \n",
       "3                        0.362663              1.0        1.0   \n",
       "4                        2.222767              1.0        1.0   \n",
       "\n",
       "   used_pin_number  online_order  fraud  \n",
       "0              0.0           0.0    0.0  \n",
       "1              0.0           0.0    0.0  \n",
       "2              0.0           1.0    0.0  \n",
       "3              0.0           1.0    0.0  \n",
       "4              0.0           1.0    0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/card_transdata.csv\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este modelo NO cambiamos las columnas a categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columnas_cat = [\"used_chip\", \"repeat_retailer\", \"online_order\", \"used_pin_number\", \"fraud\"]\n",
    "# for columna in columnas_cat:\n",
    "#     df[columna] = df[columna].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericas = df.select_dtypes(include = np.number)\n",
    "numericas.drop([\"repeat_retailer\",\"used_chip\", \"online_order\", \"used_pin_number\", \"fraud\" ], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construir el modelo de escalador\n",
    "robust = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajustamos el modelo utilizando nuestro set de datos\n",
    "robust.fit(numericas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformamos los datos\n",
    "X_robust = robust.transform(numericas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericas_robust = pd.DataFrame(X_robust, columns = numericas.columns)\n",
    "numericas_robust.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numericas_robust.columns] = numericas_robust\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"data/df_robust.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para aplicar este método lo primero que tenemos que hacer es separar en X e y y en train y test como aprendimos en la lecciones de regresion lineal\n",
    "y = df['fraud']\n",
    "X = df.drop('fraud', axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniciamos el método\n",
    "os_us = SMOTETomek()\n",
    "\n",
    "# ajustamos el modelo\n",
    "X_train_res, y_train_res = os_us.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprobemos como han quedado ahora las categorías después del ajuste\n",
    "\n",
    "print (f\"Distribution before resampling \\n {y_train.value_counts()}\" )\n",
    "print(\"..............................................................\")\n",
    "print (f\"Distribución después del ajuste \\n {y_train_res.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_res.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanceo_sin_cat = pd.concat([y_train_res, X_train_res], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanceo_sin_cat.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanceo_sin_cat['fraud'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanceo_sin_cat.to_pickle('data/df_balanceo_sin_cat.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separamos los datos en X e y\n",
    "\n",
    "X1 = df.drop(\"fraud\", axis = 1)\n",
    "y1 = df[\"fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la regresión logistica\n",
    "\n",
    "log_reg_esta = LogisticRegression(n_jobs=-1, max_iter = 1000)\n",
    "\n",
    "# ajustamos el modelo\n",
    "log_reg_esta.fit(x_train1,y_train1)\n",
    "\n",
    "# obtenemos las predicciones para el conjunto de entrenamiento\n",
    "y_pred_train_esta = log_reg_esta.predict(x_train1)\n",
    "\n",
    "# obtenemos las predicciones para el conjunto de test\n",
    "y_pred_test_esta = log_reg_esta.predict(x_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_esta = pd.DataFrame({'Real': y_train1, 'Predicted': y_pred_train_esta, 'Set': ['Train']*len(y_train1)})\n",
    "test_df_esta  = pd.DataFrame({'Real': y_test1,  'Predicted': y_pred_test_esta,  'Set': ['Test']*len(y_test1)})\n",
    "resultados = pd.concat([train_df_esta,test_df_esta], axis = 0)\n",
    "resultados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATRIZ DE CORRELACIÓN PARA DATOS CODIFICADOS Y  ESTANDARIZADOS\n",
    "\n",
    "mat_lr1 = confusion_matrix(y_test1, y_pred_test_esta)\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "sns.heatmap(mat_lr1, square=True, annot=True, fmt=\"d\", cmap = \"viridis\")\n",
    "\n",
    "plt.xlabel('valor predicho')\n",
    "plt.ylabel('valor real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separamos los datos en X e y\n",
    "\n",
    "X2 = df_balanceo_sin_cat.drop(\"fraud\", axis = 1)\n",
    "y2 = df_balanceo_sin_cat[\"fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la regresión logistica\n",
    "\n",
    "log_reg = LogisticRegression(n_jobs=-1, max_iter = 1000)\n",
    "\n",
    "# ajustamos el modelo\n",
    "log_reg.fit(x_train2,y_train2)\n",
    "\n",
    "# obtenemos las predicciones para el conjunto de entrenamiento\n",
    "y_pred_train = log_reg_esta.predict(x_train2)\n",
    "\n",
    "# obtenemos las predicciones para el conjunto de test\n",
    "y_pred_test = log_reg_esta.predict(x_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATRIZ DE CORRELACIÓN PARA DATOS CODIFICADOS Y SIN ESTANDARIZAR\n",
    "\n",
    "mat_lr2 = confusion_matrix(y_test2, y_pred_test)\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "sns.heatmap(mat_lr2, square=True, annot=True, fmt=\"d\", cmap = \"viridis\")\n",
    "\n",
    "plt.xlabel('valor predicho')\n",
    "plt.ylabel('valor real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a hacernos una función para sacar las métricas igual que hicimos con al regresión lineal.\n",
    "\n",
    "\n",
    "def metricas(clases_reales_test, clases_predichas_test, clases_reales_train, clases_predichas_train, modelo):\n",
    "    \n",
    "    # para el test\n",
    "    accuracy_test = accuracy_score(clases_reales_test, clases_predichas_test)\n",
    "    precision_test = precision_score(clases_reales_test, clases_predichas_test)\n",
    "    recall_test = recall_score(clases_reales_test, clases_predichas_test)\n",
    "    f1_test = f1_score(clases_reales_test, clases_predichas_test)\n",
    "    kappa_test = cohen_kappa_score(clases_reales_test, clases_predichas_test)\n",
    "\n",
    "    # para el train\n",
    "    accuracy_train = accuracy_score(clases_reales_train, clases_predichas_train)\n",
    "    precision_train = precision_score(clases_reales_train, clases_predichas_train)\n",
    "    recall_train = recall_score(clases_reales_train, clases_predichas_train)\n",
    "    f1_train = f1_score(clases_reales_train, clases_predichas_train)\n",
    "    kappa_train = cohen_kappa_score(clases_reales_train, clases_predichas_train)\n",
    "    \n",
    "\n",
    "    \n",
    "    df = pd.DataFrame({\"accuracy\": [accuracy_test, accuracy_train], \n",
    "                       \"precision\": [precision_test, precision_train],\n",
    "                       \"recall\": [recall_test, recall_train], \n",
    "                       \"f1\": [f1_test, f1_train],\n",
    "                       \"kapppa\": [kappa_test, kappa_train],\n",
    "                       \"set\": [\"test\", \"train\"]})\n",
    "    \n",
    "    df[\"modelo\"] = modelo\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_logistic_esta = metricas(y_test1, y_pred_test_esta, y_train1, y_pred_train_esta, \"Regresión logistica sin cat\")\n",
    "results_logistic_esta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_logistic = metricas(y_test2, y_pred_test, y_train2, y_pred_train, \"Regresión logistica balanceado sin cat\")\n",
    "results_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_sin_cat = pd.concat([results_logistic, results_logistic_esta], axis = 0)\n",
    "resultados_sin_cat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
